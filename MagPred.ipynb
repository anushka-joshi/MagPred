{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704373da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 1954,
     "status": "error",
     "timestamp": 1677955078378,
     "user": {
      "displayName": "Anushka Joshi",
      "userId": "14740637928789097697"
     },
     "user_tz": -330
    },
    "id": "704373da",
    "outputId": "76ecfef0-9ea5-4f98-ec50-3033e4e4c8d6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "from scipy import stats\n",
    "import numpy.random as rd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import catboost as ctb\n",
    "from sklearn.metrics import mean_squared_error as MSE \n",
    "from time import time\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb \n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from ctgan import CTGAN\n",
    "import torch\n",
    "import sklearn.model_selection\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fc9c41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1678010211317,
     "user": {
      "displayName": "Anushka Joshi",
      "userId": "14740637928789097697"
     },
     "user_tz": -330
    },
    "id": "25fc9c41",
    "outputId": "787c881e-ba60-4a24-af81-57aa60a88b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input test set (49, 34)\n"
     ]
    }
   ],
   "source": [
    "def Declare_file(df_merged):\n",
    "  merged_file=df_merged                                         \n",
    "  # new_file=[f for f in merged_file.columns if f not in [\"Longitude\",\"Latitude\",\n",
    "  #                                                       \"Epicenter Distance\",\"Hypocenter Distance\",\"Station_Lat\",\n",
    "  #                                                 \"Station_Lon\",\"Epicenter_Lat\",\"Epicenter_Lon\",\"Original Time\",\n",
    "  #                                                 \"Recording Start Time\",\"Station Name\",\"Magnitude\",\n",
    "  #                                                 \"Recording Start Time\"\n",
    "                                                  \n",
    "  #                                                 ]]\n",
    "  new_file=['Vs30 Actual', 'TAU_P', 'TAU_C', 'PGV UD', \n",
    "            'PGD UD', 'pgv/pgd', 'ACF ACC 1 UD NEG', 'ACF ACC 2 UD POS',\n",
    "            'ACF VEL 1 UD NEG', 'ACF VEL 2 UD POS', 'ACF VEL 2 UD NEG',\n",
    "            'ACF DISP 1 UD POS', 'ACF DISP 1 UD NEG', 'ACF DISP 2 UD POS',\n",
    "            'ACF DISP 2 UD NEG', '0 cross 1 ACC', '0 cross 1 VEL', '0 cross 1 DISP', \n",
    "            '0 cross 2 ACC', '0 cross 2 DISP', 'ACF RATIO ACC', 'ACF RATIO DISP',\n",
    "            'VEL_ACF_1', 'VEL_ACF_2', 'VEL_ACF_3', 'VEL_ACF_5', 'VEL_ACF_6',\n",
    "            'VEL_ACF_7', 'RSSC VELOCITY', 'CA VELOCITY', 'IV^2', 'ID^2',\n",
    "            'T_VA', 'PI_v']\n",
    "  new_file.append(\"Magnitude\")\n",
    "  new_merge = merged_file[new_file]   \n",
    "  new_merge = new_merge[np.isfinite(new_merge).all(1)]\n",
    "  new_merge = new_merge.dropna()\n",
    "  return new_merge\n",
    "\n",
    "def read_Syn(Old_Test_data_fol_loc,New_Test_data_fol_loc):\n",
    "  #Reading Test\n",
    "  test_file=pd.read_csv(Old_Test_data_fol_loc)\n",
    "  test_df=Declare_file(test_file)\n",
    "  test_df.to_csv(New_Test_data_fol_loc,index=False) \n",
    "  dataset=test_df.values\n",
    "  t=dataset.shape[1]-1\n",
    "  X_test_ctf = dataset[:,0:t]\n",
    "  Y_test_ctf = dataset[:,t]\n",
    "  return X_test_ctf,Y_test_ctf\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_result(y_actual, y_pred):\n",
    "  MAE=mean_absolute_error(y_actual, y_pred)\n",
    "  print(\"Mean Absolute Error\\n\",MAE)\n",
    "  MSE=mean_squared_error(y_actual, y_pred)\n",
    "  print(\"Mean Square Error\\n\",MSE)\n",
    "  print(\"Root Mean Square Error\\n\",math.sqrt(MSE))\n",
    "  print(\"-------------------------------------\")\n",
    "\n",
    "\n",
    "def Averaging_L2(file_loc):\n",
    "  df=pd.read_csv(file_loc,sep=',')\n",
    "  actual_test=df['Magnitude']\n",
    "  LightGBM_NonSyn_test= df['LightGBMNonSyn Pred']\n",
    "  LightGBM_Syn_test= df['LightGBMSyn Pred']\n",
    "  GWO_XGB_Syn_test= df['GWOXGBSyn Pred']\n",
    "  GWO_XGB_NonSyn_test= df['GWOXGBNonSyn Pred']\n",
    "  XGB_Syn_test= df['XGBSyn Pred']\n",
    "  XGB_NonSyn_test= df['XGBNonSyn Pred']\n",
    "  CatBNonSyn_test= df['CatBNonSyn Pred']\n",
    "  CatBSyn_test= df['CatBSyn Pred']\n",
    "  RFNonSyn_test= df['RFNonSyn Pred']\n",
    "  RFSyn_test= df['RFSyn Pred']\n",
    "  avg_pred=(XGB_NonSyn_test.astype(int)+XGB_Syn_test.astype(int)+\n",
    "          GWO_XGB_NonSyn_test.astype(int)+CatBSyn_test.astype(int)+\n",
    "          LightGBM_Syn_test.astype(int)+GWO_XGB_Syn_test.astype(int)+\n",
    "          LightGBM_NonSyn_test.astype(int)+CatBNonSyn_test.astype(int)\n",
    "          )/8\n",
    "  final_mag=[]\n",
    "  for i in range(0,len(avg_pred)):\n",
    "    if avg_pred[i]>6.2:\n",
    "      final_mag.append(avg_pred[i])\n",
    "      continue\n",
    "    if avg_pred[i]<3.9 and avg_pred[i]>2.9:\n",
    "      RF_N=(RFNonSyn_test[i]+RFSyn_test[i])/2\n",
    "      final_mag.append(np.round(RF_N,1))\n",
    "      continue\n",
    "    else:\n",
    "      temp=(XGB_NonSyn_test[i]+CatBNonSyn_test[i]+GWO_XGB_NonSyn_test[i])/3\n",
    "      final_mag.append(np.round(temp,1))\n",
    "      continue\n",
    "  return final_mag\n",
    "\n",
    "def mag_mean(mag):\n",
    "  pred_mag=mag['Magnitude Pred'].values\n",
    "  sum=0\n",
    "  size=len(pred_mag)\n",
    "  for i in range(0,size):\n",
    "    sum=sum+pred_mag[i]\n",
    "  mean=sum/size\n",
    "  return mean\n",
    "\n",
    "def add_station_name(old_file,new_file,generated_file):\n",
    "  old_df=pd.read_csv(old_file)\n",
    "  new_df=pd.read_csv(new_file,sep=',')\n",
    "  #reset index to remove nan\n",
    "  old_df.reset_index(drop=True, inplace=True)\n",
    "  new_df.reset_index(drop=True, inplace=True)\n",
    "  generated_df=pd.concat([old_df, new_df], axis=1)\n",
    "  generated_df = generated_df.dropna()\n",
    "  generated_df.to_csv(generated_file,sep=',')\n",
    "\n",
    "def PGD_method(df):\n",
    "  Pd=df['PGD UD'].values\n",
    "  hy=df['Hypocenter Distance'].values\n",
    "  M= 0.91*np.log10(Pd)+0.48*np.log10(hy)+5.65\n",
    "  return np.round(M,1)\n",
    "\n",
    "def Tauc_method(df):\n",
    "  Tauc=df['TAU_C'].values\n",
    "  M=2.94*np.log10(Tauc)+5.3\n",
    "  return np.round(M,1)\n",
    "  \n",
    "def Regression_methods(file_loc):\n",
    "  df=pd.read_csv(file_loc,sep=',')\n",
    "  df['PGD Pred Mag']=PGD_method(df)\n",
    "  df['Tauc Pred Mag']=Tauc_method(df)\n",
    "  actual_mag=df['Magnitude'].values\n",
    "  df['Error PGD']=actual_mag-df['PGD Pred Mag'].values\n",
    "  df['Error Tauc']=actual_mag-df['Tauc Pred Mag'].values\n",
    "  df['Error ML Pred']=actual_mag-df['Magnitude Pred'].values\n",
    "  df.to_csv(file_loc,index=False)\n",
    "\n",
    "def mag_select(folder_loc,EQ1_loc,EQ2_loc,EQ3_loc,EQ4_loc):\n",
    "  #magn 5.8\n",
    "  df=pd.read_csv(folder_loc,sep=',')\n",
    "  #EQ1\n",
    "  df=df[df['Magnitude'] == 7.3]\n",
    "  df=df[df['Epicenter_Lat'] == 38.032]\n",
    "  df=df[df['Epicenter_Lon'] == 143.507]\n",
    "  df.to_csv(EQ1_loc,index=False)\n",
    "  #EQ2\n",
    "  df=pd.read_csv(folder_loc,sep=',')\n",
    "  df=df[df['Magnitude'] == 7]\n",
    "  df=df[df['Epicenter_Lat'] == 33.738]\n",
    "  df=df[df['Epicenter_Lon'] == 130.175]\n",
    "  df.to_csv(EQ2_loc,index=False)\n",
    "  #EQ3\n",
    "  df=pd.read_csv(folder_loc,sep=',')\n",
    "  df=df[df['Magnitude'] == 5.4]\n",
    "  df=df[df['Epicenter_Lat'] == 34.872]\n",
    "  df=df[df['Epicenter_Lon'] == 132.893]\n",
    "  df.to_csv(EQ3_loc,index=False)\n",
    "  #EQ4\n",
    "  df=pd.read_csv(folder_loc,sep=',')\n",
    "  df=df[df['Magnitude'] == 6.2]\n",
    "  df=df[df['Epicenter_Lat'] == 44.43]\n",
    "  df=df[df['Epicenter_Lon'] == 141.21]\n",
    "  df.to_csv(EQ4_loc,index=False)\n",
    "\n",
    "\n",
    "def percent_error(actual,pred):\n",
    "    per = (actual - pred) / actual\n",
    "    per_err = abs(per)\n",
    "    print(round(per_err*100, 2),\"%\")\n",
    "    \n",
    "def calc_weight_mag(eq_loc):\n",
    "    df=pd.read_csv(eq_loc,sep=\",\")\n",
    "    df = df.dropna()\n",
    "    station_time=df['Recording Start Time']\n",
    "    eathquake_time=df['Original Time']\n",
    "    mag=df['Magnitude Pred']\n",
    "    print(\"Total Magnitudes are:\\n\",mag.shape)\n",
    "    #calculating time of origin of earthquake\n",
    "    date_time = datetime.strptime(eathquake_time[0], \"%H:%M:%S\")\n",
    "    a_timedelta = date_time - datetime(1900, 1, 1)\n",
    "    earth_seconds = a_timedelta.total_seconds()\n",
    "    print(\"Earthquake Seconds\\n\",earth_seconds)\n",
    "    #start time\n",
    "    start_set=np.zeros(len(mag))\n",
    "    for i in range(0,len(mag)):\n",
    "      date_time = datetime.strptime(station_time[i], \"%H:%M:%S\")\n",
    "      a_timedelta = date_time - datetime(1900, 1, 1)\n",
    "      station_seconds = a_timedelta.total_seconds()\n",
    "      st_time_diff=station_seconds-earth_seconds\n",
    "      start_set[i]=st_time_diff\n",
    "    start_time=np.amin(start_set)\n",
    "    end_time=np.amax(start_set)\n",
    "    print(\"End Time\\n\", end_time)\n",
    "    print(\"Start Time\\n\", start_time)\n",
    "    max_j=np.round(end_time/3.0,0)+1\n",
    "    final_mag=np.zeros(int(max_j))\n",
    "    count_mag=np.zeros(int(max_j))\n",
    "    count=0\n",
    "    for i in range(0,len(mag)):\n",
    "      st_time_diff=start_set[i]\n",
    "      for j in range(1,int(max_j)):\n",
    "        k=3*j\n",
    "        if st_time_diff <k:\n",
    "          final_mag[j]=final_mag[j]+mag[i]\n",
    "          count_mag[j]=count_mag[j]+1\n",
    "          break\n",
    "    final_mag=final_mag/count_mag\n",
    "    #check for nan\n",
    "    nan_final_mag = np.isnan(final_mag)\n",
    "    not_nan_final_mag = ~ nan_final_mag\n",
    "    final_mag = final_mag[not_nan_final_mag]\n",
    "    weighted_final_mag=0\n",
    "    unweighted_final_mag=0\n",
    "    for i in range(0,len(final_mag)):\n",
    "      w=1.005+0.01*i\n",
    "      weighted_final_mag=weighted_final_mag+w*final_mag[0]\n",
    "      unweighted_final_mag=unweighted_final_mag+final_mag[0]\n",
    "    weighted_final_mag=weighted_final_mag/len(final_mag)\n",
    "    weighted_final_mag=np.round(weighted_final_mag,1)\n",
    "    print(\"Weighted Final Magnitude\",weighted_final_mag)\n",
    "    unweighted_final_mag=unweighted_final_mag/len(final_mag)\n",
    "    unweighted_final_mag=np.round(unweighted_final_mag,1)\n",
    "    print(\"Unweighted result\")\n",
    "    print(unweighted_final_mag)\n",
    "    print('Actual Magnitude')\n",
    "    print(df['Magnitude'][0])\n",
    "    print(\"Percentage error for unweighted magnitude:\")\n",
    "    percent_error(df['Magnitude'][0],unweighted_final_mag)\n",
    "    print(\"Percentage error for weighted magnitude:\")\n",
    "    percent_error(df['Magnitude'][0],weighted_final_mag)\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "\n",
    "def Statistical_eval(file_loc):\n",
    "  df=pd.read_csv(file_loc,sep=',')\n",
    "  actual_test=df['Magnitude']\n",
    "  LightGBM_NonSyn_test= df['LightGBMNonSyn Pred']\n",
    "  LightGBM_Syn_test= df['LightGBMSyn Pred']\n",
    "  GWO_XGB_Syn_test= df['GWOXGBSyn Pred']\n",
    "  GWO_XGB_NonSyn_test= df['GWOXGBNonSyn Pred']\n",
    "  XGB_Syn_test= df['XGBSyn Pred']\n",
    "  XGB_NonSyn_test= df['XGBNonSyn Pred']\n",
    "  CatBNonSyn_test= df['CatBNonSyn Pred']\n",
    "  CatBSyn_test= df['CatBSyn Pred']\n",
    "  RFNonSyn_test= df['RFNonSyn Pred']\n",
    "  RFSyn_test= df['RFSyn Pred']\n",
    "  ML_test= df['Magnitude Pred']\n",
    "  print(\"MagPred Result\\n\")\n",
    "  evaluate_result(actual_test, ML_test)\n",
    "  RFNonSyn_test= df['PGD Pred Mag']\n",
    "  print(\"Predicted Magnitude using PGD regression relation : \\n \")\n",
    "  evaluate_result(actual_test, RFNonSyn_test) \n",
    "  RFSyn_test= df['Tauc Pred Mag']\n",
    "  print(\"Predicted Magnitude using Tauc regression relation : \\n \")\n",
    "  evaluate_result(actual_test, RFSyn_test)\n",
    "  ML_test= df['Magnitude Pred']\n",
    "  print(\"Predicted Magnitude using MagPred model :\\n \")\n",
    "  evaluate_result(actual_test, ML_test)\n",
    "\n",
    "#Declaring File locations\n",
    "Old_Test_data_fol_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/Test_Dataset.csv'\n",
    "New_Test_data_fol_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/Test_Dataset_Modified.csv'\n",
    "Result_Test_fol_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/Test_Result.csv'\n",
    "EQ1_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/EQR1.csv'\n",
    "EQ2_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/EQR2.csv'\n",
    "EQ3_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/EQR3.csv'\n",
    "EQ4_loc='~/notebook/Paper 6/Work paper 6/Model_Wise_Result/Github Data/EQR4.csv'\n",
    "\n",
    "#Reading the Built file\n",
    "X_test_ctf,Y_test_ctf=read_Syn(Old_Test_data_fol_loc,New_Test_data_fol_loc)\n",
    "print('shape of input test set',X_test_ctf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b91eb73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1678010144722,
     "user": {
      "displayName": "Anushka Joshi",
      "userId": "14740637928789097697"
     },
     "user_tz": -330
    },
    "id": "7b91eb73",
    "outputId": "f0630aa7-fc37-4c48-c02e-a3a1922d73b4"
   },
   "outputs": [],
   "source": [
    "XGB_NonSyn=joblib.load('Work paper 6/Models/XGB_NonSyn.pkl')\n",
    "XGB_Syn=joblib.load('Work paper 6/Models/XGB_Syn.pkl')\n",
    "CatB_NonSyn=joblib.load('Work paper 6/Models/CatB_NonSyn.pkl')\n",
    "CatB_Syn=joblib.load('Work paper 6/Models/CatB_Syn.pkl')\n",
    "LightGBM_NonSyn=joblib.load('Work paper 6/Models/LightGBM_NonSyn.pkl')\n",
    "LightGBM_Syn=joblib.load('Work paper 6/Models/LightGBM_Syn.pkl')\n",
    "GWO_XGB_NonSyn=joblib.load('Work paper 6/Models/GWO_XGB_NonSyn.pkl')\n",
    "GWO_XGB_Syn=joblib.load('Work paper 6/Models/GWO_XGB_Syn.pkl')\n",
    "RF_1_NonSyn=joblib.load('Work paper 6/Models/RF_1_NonSyn.pkl')\n",
    "RF_1_Syn=joblib.load('Work paper 6/Models/RF_1_Syn.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0832a837",
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1678010279731,
     "user": {
      "displayName": "Anushka Joshi",
      "userId": "14740637928789097697"
     },
     "user_tz": -330
    },
    "id": "0832a837"
   },
   "outputs": [],
   "source": [
    "def Testing_on_Dataset(GWO_XGB_NonSyn,GWO_XGB_Syn,XGB_NonSyn,XGB_Syn,CatB_NonSyn,CatB_Syn,LightGBM_NonSyn,LightGBM_Syn,\n",
    "                       RF_1_NonSyn,RF_1_Syn,X_test_ctf,Y_test_ctf,EQ1_loc,EQ2_loc,EQ3_loc,EQ4_loc,\n",
    "                       New_Test_data_fol_loc,Result_fol_loc,eq_flag):\n",
    "  #SAving result in other folder\n",
    "  df=pd.read_csv(New_Test_data_fol_loc,sep=',')\n",
    "  df.to_csv(Result_fol_loc,index=False)\n",
    "  test_df=pd.read_csv(Result_fol_loc,sep=',')\n",
    "  print(\"\\nRESULT\\n\")\n",
    "  y_pred_GWO_XGB_NonSyn=GWO_XGB_NonSyn.predict(X_test_ctf)\n",
    "  y_pred_GWO_XGB_Syn=GWO_XGB_Syn.predict(X_test_ctf)\n",
    "  test_df['GWOXGBNonSyn Pred']=np.round(y_pred_GWO_XGB_NonSyn, 1)\n",
    "  test_df['GWOXGBSyn Pred']=np.round(y_pred_GWO_XGB_Syn, 1)\n",
    "  y_pred_xgb_NonSyn=XGB_NonSyn.predict(X_test_ctf)\n",
    "  y_pred_xgb_Syn=XGB_Syn.predict(X_test_ctf)\n",
    "  test_df['XGBNonSyn Pred']=np.round(y_pred_xgb_NonSyn, 1)\n",
    "  test_df['XGBSyn Pred']=np.round(y_pred_xgb_Syn, 1)\n",
    "  y_pred_RF_1_NonSyn=RF_1_NonSyn.predict(X_test_ctf)\n",
    "  y_pred_RF_1_Syn=RF_1_Syn.predict(X_test_ctf)\n",
    "  test_df['RFNonSyn Pred']=np.round(y_pred_RF_1_NonSyn, 1)\n",
    "  test_df['RFSyn Pred']=np.round(y_pred_RF_1_Syn, 1)\n",
    "  y_pred_CatB_NonSyn=CatB_NonSyn.predict(X_test_ctf)\n",
    "  y_pred_CatB_Syn=CatB_Syn.predict(X_test_ctf)\n",
    "  test_df['CatBNonSyn Pred']=np.round(y_pred_CatB_NonSyn, 1)\n",
    "  test_df['CatBSyn Pred']=np.round(y_pred_CatB_Syn, 1)\n",
    "  y_pred_LightGBM_NonSyn=LightGBM_NonSyn.predict(X_test_ctf,num_iteration=LightGBM_NonSyn.best_iteration)\n",
    "  y_pred_LightGBM_Syn=LightGBM_Syn.predict(X_test_ctf,num_iteration=LightGBM_Syn.best_iteration)\n",
    "  test_df['LightGBMNonSyn Pred']=np.round(y_pred_LightGBM_NonSyn, 1)\n",
    "  test_df['LightGBMSyn Pred']=np.round(y_pred_LightGBM_Syn, 1)\n",
    "  test_df.to_csv(Result_fol_loc,index=False)\n",
    "  final_mag=Averaging_L2(Result_fol_loc)\n",
    "  final_mag=np.array(final_mag)\n",
    "  test_df['Magnitude Pred']=np.round(final_mag, 1)\n",
    "  test_df.to_csv(Result_fol_loc,index=False)\n",
    "  add_station_name(Old_Test_data_fol_loc,Result_fol_loc,Result_fol_loc)\n",
    "  Regression_methods(Result_fol_loc)\n",
    "  mag_select(Result_fol_loc,EQ1_loc,EQ2_loc,EQ3_loc,EQ4_loc)\n",
    "  Statistical_eval(Result_fol_loc)\n",
    "  if eq_flag==1:\n",
    "      #Earthquake 1\n",
    "      print(\"EARTHQUAKE 1\")\n",
    "      calc_weight_mag(EQ1_loc)\n",
    "      #Earthquake 2\n",
    "      print(\"EARTHQUAKE 2\")\n",
    "      calc_weight_mag(EQ2_loc)\n",
    "      #Earthquake 3\n",
    "      print(\"EARTHQUAKE 3\")\n",
    "      calc_weight_mag(EQ3_loc)\n",
    "      #Earthquake \n",
    "      print(\"EARTHQUAKE 4\")\n",
    "      calc_weight_mag(EQ4_loc)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aae2f03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678010281986,
     "user": {
      "displayName": "Anushka Joshi",
      "userId": "14740637928789097697"
     },
     "user_tz": -330
    },
    "id": "9aae2f03",
    "outputId": "ae00eac6-e49e-44a5-cd43-580d8de5d586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT\n",
      "\n",
      "MagPred Result\n",
      "\n",
      "Mean Absolute Error\n",
      " 0.7142857142857143\n",
      "Mean Square Error\n",
      " 0.8016326530612246\n",
      "Root Mean Square Error\n",
      " 0.895339406628137\n",
      "-------------------------------------\n",
      "Predicted Magnitude using PGD regression relation : \n",
      " \n",
      "Mean Absolute Error\n",
      " 2.0693877551020408\n",
      "Mean Square Error\n",
      " 4.700816326530612\n",
      "Root Mean Square Error\n",
      " 2.168136602368636\n",
      "-------------------------------------\n",
      "Predicted Magnitude using Tauc regression relation : \n",
      " \n",
      "Mean Absolute Error\n",
      " 2.8020408163265307\n",
      "Mean Square Error\n",
      " 8.570816326530613\n",
      "Root Mean Square Error\n",
      " 2.9275956562562757\n",
      "-------------------------------------\n",
      "Predicted Magnitude using MagPred model :\n",
      " \n",
      "Mean Absolute Error\n",
      " 0.7142857142857143\n",
      "Mean Square Error\n",
      " 0.8016326530612246\n",
      "Root Mean Square Error\n",
      " 0.895339406628137\n",
      "-------------------------------------\n",
      "EARTHQUAKE 1\n",
      "Total Magnitudes are:\n",
      " (21,)\n",
      "Earthquake Seconds\n",
      " 35820.0\n",
      "End Time\n",
      " 57.0\n",
      "Start Time\n",
      " 36.0\n",
      "Weighted Final Magnitude 7.0\n",
      "Unweighted result\n",
      "6.8\n",
      "Actual Magnitude\n",
      "7.3\n",
      "Percentage error for unweighted magnitude:\n",
      "6.85 %\n",
      "Percentage error for weighted magnitude:\n",
      "4.11 %\n",
      "-------------------------------------\n",
      "EARTHQUAKE 2\n",
      "Total Magnitudes are:\n",
      " (9,)\n",
      "Earthquake Seconds\n",
      " 39180.0\n",
      "End Time\n",
      " 57.0\n",
      "Start Time\n",
      " 50.0\n",
      "Weighted Final Magnitude 6.3\n",
      "Unweighted result\n",
      "6.2\n",
      "Actual Magnitude\n",
      "7.0\n",
      "Percentage error for unweighted magnitude:\n",
      "11.43 %\n",
      "Percentage error for weighted magnitude:\n",
      "10.0 %\n",
      "-------------------------------------\n",
      "EARTHQUAKE 3\n",
      "Total Magnitudes are:\n",
      " (6,)\n",
      "Earthquake Seconds\n",
      " 69360.0\n",
      "End Time\n",
      " 41.0\n",
      "Start Time\n",
      " 35.0\n",
      "Weighted Final Magnitude 5.7\n",
      "Unweighted result\n",
      "5.6\n",
      "Actual Magnitude\n",
      "5.4\n",
      "Percentage error for unweighted magnitude:\n",
      "3.7 %\n",
      "Percentage error for weighted magnitude:\n",
      "5.56 %\n",
      "-------------------------------------\n",
      "EARTHQUAKE 4\n",
      "Total Magnitudes are:\n",
      " (13,)\n",
      "Earthquake Seconds\n",
      " 7680.0\n",
      "End Time\n",
      " 57.0\n",
      "Start Time\n",
      " 37.0\n",
      "Weighted Final Magnitude 6.4\n",
      "Unweighted result\n",
      "6.4\n",
      "Actual Magnitude\n",
      "6.2\n",
      "Percentage error for unweighted magnitude:\n",
      "3.23 %\n",
      "Percentage error for weighted magnitude:\n",
      "3.23 %\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172666/768276120.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  final_mag=final_mag/count_mag\n",
      "/tmp/ipykernel_172666/768276120.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  final_mag=final_mag/count_mag\n",
      "/tmp/ipykernel_172666/768276120.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  final_mag=final_mag/count_mag\n",
      "/tmp/ipykernel_172666/768276120.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  final_mag=final_mag/count_mag\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "Testing_on_Dataset(GWO_XGB_NonSyn,GWO_XGB_Syn,XGB_NonSyn,XGB_Syn,CatB_NonSyn,CatB_Syn,LightGBM_NonSyn,LightGBM_Syn,\n",
    "                       RF_1_NonSyn,RF_1_Syn,X_test_ctf,Y_test_ctf,EQ1_loc,EQ2_loc,EQ3_loc,EQ4_loc,\n",
    "                       New_Test_data_fol_loc,Result_Test_fol_loc,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32674b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
